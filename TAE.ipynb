{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "import torch\n",
    "\n",
    "class TransformerEncoder(torch.nn.Module):\n",
    "    def __init__(self, num_layers: int, num_heads: int, dim: int, time: int, mz: int):\n",
    "        super().__init__()\n",
    "        self.pos = torch.nn.Embedding(time+1, dim)\n",
    "        self.proj = torch.nn.Linear(mz, dim)\n",
    "        self.cls = torch.nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.encoder = torch.nn.TransformerEncoder(torch.nn.TransformerEncoderLayer(d_model=dim, nhead=num_heads, batch_first=True), num_layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        b, _, _ = x.shape\n",
    "        x = self.proj(x)\n",
    "        cls = self.cls.expand(b, -1, -1)\n",
    "        x = torch.cat((cls, x), dim=1)\n",
    "        x += self.pos(torch.arange(x.shape[1], device=x.device))\n",
    "        x = self.encoder(x)\n",
    "        return x[:,0,:]\n",
    "encoder = TransformerEncoder(8, 8, 1024, 1024, 1024)\n",
    "assert encoder(torch.randn(1, 1024, 1024)).shape == (1, 1024)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(torch.nn.Module):\n",
    "    def __init__(self, num_layers: int, num_heads: int, dim: int, time: int, mz: int):\n",
    "        super().__init__()\n",
    "        self.time = time\n",
    "        self.mz = mz\n",
    "\n",
    "        self.pos = torch.nn.Embedding(time+1, dim)\n",
    "        self.out_put_proj = torch.nn.Linear(dim, mz)\n",
    "        self.decoder = torch.nn.TransformerEncoder(torch.nn.TransformerEncoderLayer(d_model=dim, nhead=num_heads, batch_first=True), num_layers)\n",
    "\n",
    "    def forward(self, latten: torch.Tensor):\n",
    "        pos = self.pos(torch.arange(self.time+1, device=latten.device))\n",
    "        pos = pos.unsqueeze(0).expand(latten.shape[0], -1, -1)\n",
    "        pos[:,0,:] += latten\n",
    "        x = self.decoder(pos)\n",
    "        return self.out_put_proj(x)[:,1:,:]\n",
    "decoder = TransformerDecoder(8, 8, 1024, 1024, 1024)\n",
    "assert decoder(torch.randn(1, 1024)).shape == (1, 1024, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, num_layers: int, num_heads: int, dim: int, time: int, mz: int):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(num_layers, num_heads, dim, time, mz)\n",
    "        self.decoder = TransformerDecoder(num_layers, num_heads, dim, time, mz)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.decoder(self.encoder(x))\n",
    "model = Model(8, 8, 1024, 1024, 1024)\n",
    "assert model(torch.randn(1, 1024, 1024)).shape == (1, 1024, 1024)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
